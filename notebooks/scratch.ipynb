{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e476cac3-e3df-4cb9-b4c3-fa0bd7ccf53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src = str(Path('../src').resolve())\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)\n",
    "    \n",
    "import os\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e09d6d-62c7-4ae9-a4d9-34ef2f1a605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import equinox as eqx\n",
    "\n",
    "def generate_synthetic_data(batch_size, static_in_size, daily_in_size, irregular_in_size, seq_length):\n",
    "    x_s = jax.random.normal(jax.random.PRNGKey(0), (batch_size, static_in_size))\n",
    "    x_dd = jax.random.normal(jax.random.PRNGKey(1), (batch_size, seq_length, daily_in_size))\n",
    "    x_di = jax.random.normal(jax.random.PRNGKey(2), (batch_size, seq_length, irregular_in_size))\n",
    "\n",
    "    data = {\n",
    "        'x_s': x_s,\n",
    "        'x_dd': x_dd,\n",
    "        'x_di': x_di\n",
    "    }    \n",
    "    return data\n",
    "\n",
    "batch_size = 8\n",
    "static_in_size = 14\n",
    "daily_in_size = 22  \n",
    "irregular_in_size = 12  \n",
    "seq_length = 365\n",
    "hidden_size = 64\n",
    "out_size = 4\n",
    "num_heads = 4\n",
    "dropout = 0.5\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = generate_synthetic_data(batch_size, static_in_size, daily_in_size, irregular_in_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28e8003-cd2a-43ee-b115-b9f619b9018d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 286,084 parameters, using 1.09MB memory.\n",
      "Output shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from importlib import reload\n",
    "reload(models)\n",
    "from models.tft_mha import TFT_MHA\n",
    "\n",
    "# Initialize the TFT model\n",
    "dynamic_sizes = {'x_dd': daily_in_size, 'x_di': irregular_in_size}\n",
    "model = TFT_MHA(\n",
    "    dynamic_sizes = dynamic_sizes,\n",
    "    static_size = static_in_size,\n",
    "    hidden_size=hidden_size,\n",
    "    out_size=out_size,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "num_params, memory_bytes = models.count_parameters(model)\n",
    "size, unit = models.human_readable_size(memory_bytes)\n",
    "print(f\"Model contains {num_params:,} parameters, using {size:.2f}{unit} memory.\")\n",
    "\n",
    "# Run one step of the model\n",
    "key = jax.random.PRNGKey(0)\n",
    "batch_keys = jax.random.split(key, batch_size)\n",
    "output = jax.vmap(model)(synthetic_data, batch_keys)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fc4249-9207-42fa-8eea-5f1df683f76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridEncoder(\n",
       "  dynamic_blocks={\n",
       "    'x_dd':\n",
       "    HybridEncoderBlock(\n",
       "      head_context=StaticContextHeadBias(\n",
       "        out_shape=(4, 16),\n",
       "        linear=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=f32[64],\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=True\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        layernorm=LayerNorm(\n",
       "          shape=(4, 16),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=f32[4,16],\n",
       "          bias=f32[4,16]\n",
       "        )\n",
       "      ),\n",
       "      dynamic_proj=Linear(\n",
       "        weight=f32[64,22],\n",
       "        bias=f32[64],\n",
       "        in_features=22,\n",
       "        out_features=64,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      attn=MultiheadAttention(\n",
       "        query_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        key_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        value_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        output_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        num_heads=4,\n",
       "        query_size=64,\n",
       "        key_size=64,\n",
       "        value_size=64,\n",
       "        output_size=64,\n",
       "        qk_size=16,\n",
       "        vo_size=16,\n",
       "        use_query_bias=False,\n",
       "        use_key_bias=False,\n",
       "        use_value_bias=False,\n",
       "        use_output_bias=False\n",
       "      ),\n",
       "      lstm=EALSTM(\n",
       "        hidden_size=64,\n",
       "        cell=EALSTMCell(\n",
       "          weight_ih=f32[192,64],\n",
       "          weight_hh=f32[192,64],\n",
       "          bias=f32[192],\n",
       "          input_linear=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        dense=None,\n",
       "        return_all=True\n",
       "      ),\n",
       "      skip=GatedSkipLayer(\n",
       "        glu=GatedLinearUnit(\n",
       "          gates=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          linear=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        layer_norm=LayerNorm(\n",
       "          shape=(64,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=f32[64],\n",
       "          bias=f32[64]\n",
       "        )\n",
       "      )\n",
       "    ),\n",
       "    'x_di':\n",
       "    HybridEncoderBlock(\n",
       "      head_context=StaticContextHeadBias(\n",
       "        out_shape=(4, 16),\n",
       "        linear=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=f32[64],\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=True\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        layernorm=LayerNorm(\n",
       "          shape=(4, 16),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=f32[4,16],\n",
       "          bias=f32[4,16]\n",
       "        )\n",
       "      ),\n",
       "      dynamic_proj=Linear(\n",
       "        weight=f32[64,12],\n",
       "        bias=f32[64],\n",
       "        in_features=12,\n",
       "        out_features=64,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      attn=MultiheadAttention(\n",
       "        query_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        key_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        value_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        output_proj=Linear(\n",
       "          weight=f32[64,64],\n",
       "          bias=None,\n",
       "          in_features=64,\n",
       "          out_features=64,\n",
       "          use_bias=False\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        num_heads=4,\n",
       "        query_size=64,\n",
       "        key_size=64,\n",
       "        value_size=64,\n",
       "        output_size=64,\n",
       "        qk_size=16,\n",
       "        vo_size=16,\n",
       "        use_query_bias=False,\n",
       "        use_key_bias=False,\n",
       "        use_value_bias=False,\n",
       "        use_output_bias=False\n",
       "      ),\n",
       "      lstm=EALSTM(\n",
       "        hidden_size=64,\n",
       "        cell=EALSTMCell(\n",
       "          weight_ih=f32[192,64],\n",
       "          weight_hh=f32[192,64],\n",
       "          bias=f32[192],\n",
       "          input_linear=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        dropout=Dropout(p=0.5, inference=False),\n",
       "        dense=None,\n",
       "        return_all=True\n",
       "      ),\n",
       "      skip=GatedSkipLayer(\n",
       "        glu=GatedLinearUnit(\n",
       "          gates=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          linear=Linear(\n",
       "            weight=f32[64,64],\n",
       "            bias=f32[64],\n",
       "            in_features=64,\n",
       "            out_features=64,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        layer_norm=LayerNorm(\n",
       "          shape=(64,),\n",
       "          eps=1e-05,\n",
       "          use_weight=True,\n",
       "          use_bias=True,\n",
       "          weight=f32[64],\n",
       "          bias=f32[64]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  },\n",
       "  weights_grn=GatedResidualNetwork(\n",
       "    eta2_dynamic=Linear(\n",
       "      weight=f32[128,128],\n",
       "      bias=f32[128],\n",
       "      in_features=128,\n",
       "      out_features=128,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    eta2_static=Linear(\n",
       "      weight=f32[128,64],\n",
       "      bias=None,\n",
       "      in_features=64,\n",
       "      out_features=128,\n",
       "      use_bias=False\n",
       "    ),\n",
       "    eta1_linear=Linear(\n",
       "      weight=f32[128,128],\n",
       "      bias=f32[128],\n",
       "      in_features=128,\n",
       "      out_features=128,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    dropout=Dropout(p=0.5, inference=False),\n",
       "    skip=GatedSkipLayer(\n",
       "      glu=GatedLinearUnit(\n",
       "        gates=Linear(\n",
       "          weight=f32[128,128],\n",
       "          bias=f32[128],\n",
       "          in_features=128,\n",
       "          out_features=128,\n",
       "          use_bias=True\n",
       "        ),\n",
       "        linear=Linear(\n",
       "          weight=f32[128,128],\n",
       "          bias=f32[128],\n",
       "          in_features=128,\n",
       "          out_features=128,\n",
       "          use_bias=True\n",
       "        )\n",
       "      ),\n",
       "      layer_norm=LayerNorm(\n",
       "        shape=(128,),\n",
       "        eps=1e-05,\n",
       "        use_weight=True,\n",
       "        use_bias=True,\n",
       "        weight=f32[128],\n",
       "        bias=f32[128]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ff3263-584c-4d99-80de-e829eefbcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodel contains 1,717,248 parameters (54.21% of model)\n"
     ]
    }
   ],
   "source": [
    "submodel = model.encoder.dynamic_blocks\n",
    "\n",
    "sub_num_params, memory_bytes = models.count_parameters(submodel)\n",
    "print(f\"Submodel contains {sub_num_params:,} parameters ({sub_num_params/float(num_params)*100:0.2f}% of model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86fc19-0da0-4831-911c-4f5943b59814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d242e-cc20-4823-8094-db44f7a7787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(eqx.Module):\n",
    "    gates: eqx.nn.Linear\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, *, key):\n",
    "        keys = jrandom.split(key)\n",
    "        self.gates = eqx.nn.Linear(input_size, output_size, key=keys[0])\n",
    "        self.linear = eqx.nn.Linear(input_size, output_size, key=keys[1])\n",
    "\n",
    "    def __call__(self, gamma):\n",
    "        gates = jax.nn.sigmoid(self.gates(gamma))\n",
    "        return gates * self.linear(gamma)\n",
    "\n",
    "class GatedResidualNetwork(eqx.Module):\n",
    "    eta2_dynamic: eqx.nn.Linear\n",
    "    eta2_static: eqx.nn.Linear\n",
    "    eta2_bias: jnp.ndarray\n",
    "    eta1_linear: eqx.nn.Linear\n",
    "    glu: GatedLinearUnit\n",
    "    layer_norm: eqx.nn.LayerNorm\n",
    "\n",
    "    def __init__(self, grn_size, context_size=None, *, key):\n",
    "        if isinstance(grn_size, tuple):\n",
    "            input_size, hidden_size, output_size = grn_size\n",
    "        elif isinstance(grn_size, int):\n",
    "            input_size = hidden_size = output_size = grn_size\n",
    "        else:\n",
    "            raise ValueError(\"grn_size must either be a tuple or int for input, hidden, and output sizes\")\n",
    "            \n",
    "        keys = jax.random.split(key, 5)\n",
    "        self.eta2_dynamic = eqx.nn.Linear(input_size, hidden_size, use_bias=False, key=keys[0])\n",
    "        if context_size is not None:\n",
    "            self.eta2_static = eqx.nn.Linear(context_size, hidden_size, use_bias=False, key=keys[1])\n",
    "        else:\n",
    "            self.eta2_static = None\n",
    "        self.eta2_bias = jax.random.uniform(keys[2], (hidden_size,))\n",
    "\n",
    "        self.eta1_linear = eqx.nn.Linear(hidden_size, output_size, key=keys[3])\n",
    "        self.glu = GatedLinearUnit(hidden_size, hidden_size, key=keys[4])\n",
    "        self.layer_norm = eqx.nn.LayerNorm(output_size)\n",
    "\n",
    "    def __call__(self, input: jnp.ndarray, context: jnp.ndarray = None) -> jnp.ndarray:\n",
    "        if self.eta2_static and context is not None:\n",
    "            context_term = self.eta2_static(context)\n",
    "        elif self.eta2_static or context is not None:\n",
    "            raise ValueError(\"Either eta2_static was created and no context was passed during call, \" +\n",
    "                             \"or context was passed during call with no eta2_static created during init.\")\n",
    "        else:\n",
    "            context_term = 0\n",
    "        eta2 = jax.nn.elu(self.eta2_dynamic(input) + context_term + self.eta2_bias)\n",
    "        eta1 = self.eta1_linear(eta2)\n",
    "        return self.layer_norm(input + self.glu(eta1))\n",
    "    \n",
    "    \n",
    "class VariableSelectionNetwork(eqx.Module):\n",
    "    variable_transformers: list\n",
    "    variable_processors: list\n",
    "    weights_grn: GatedResidualNetwork\n",
    "    \n",
    "    def __init__(self, hidden_size, num_variables, context_size=None, key=None):\n",
    "        keys = jax.random.split(key, 4)   \n",
    "\n",
    "        transformer_keys = jax.random.split(keys[0], num_variables)\n",
    "        self.variable_transformers = [\n",
    "            eqx.nn.Linear(1, hidden_size, key=k)\n",
    "            for k in transformer_keys\n",
    "        ]\n",
    "\n",
    "        processor_keys = jax.random.split(keys[1], num_variables)\n",
    "        self.variable_processors = [\n",
    "            GatedResidualNetwork(hidden_size, key=k)\n",
    "            for k in processor_keys\n",
    "        ]\n",
    "\n",
    "        weights_size = num_variables*hidden_size\n",
    "        self.weights_grn = GatedResidualNetwork(weights_size, context_size, key=keys[2])\n",
    "        \n",
    "    \n",
    "    def __call__(self, inputs, context=None):\n",
    "        # inputs shape: (seq_length, num_variables)\n",
    "        seq_length, num_variables = inputs.shape\n",
    "        \n",
    "        transformed_inputs = jnp.stack([\n",
    "            jax.vmap(processor)(inputs[:, i][:,jnp.newaxis])\n",
    "            for i, processor in enumerate(self.variable_transformers)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Process each variable\n",
    "        processed_inputs = jnp.stack([\n",
    "            jax.vmap(processor)(transformed_inputs[:, i], None)\n",
    "            for i, processor in enumerate(self.variable_processors)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Generate variable selection weights\n",
    "        flattened = processed_inputs.reshape([seq_length,-1])\n",
    "        flat_weights = jax.vmap(self.weights_grn)(flattened, context)\n",
    "        flat_weights = jax.nn.softmax(flat_weights, axis=-1)\n",
    "        variable_weights = flat_weights.reshape(transformed_inputs.shape)\n",
    "        \n",
    "        # Weight and sum the processed inputs\n",
    "        weighted_inputs = variable_weights * processed_inputs\n",
    "        outputs = jnp.sum(weighted_inputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f9f5d-5fe7-46a4-bc4d-831742c6afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data['x_s'][0,:][jnp.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb99d0-057e-4456-a91f-95fae16ff5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5664c-dd8f-4f5b-af40-a5cba01790c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf71929-6f4d-402e-9db6-a6eab762f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e451e-d23f-455c-a902-ff6345fef333",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_vsn.weights_grn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf18533-8bc1-4ce0-87b1-6c7ccff5bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data['x_s'][0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3d1bf-b1dd-4c9c-97bf-522efc072a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_out[0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686659a-b9c5-4d4f-bf17-b79fecee5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "batch_xs = synthetic_data['x_s'][0,...]\n",
    "static_vsn = VariableSelectionNetwork(hidden_size, static_in_size, key=key)\n",
    "static_out = static_vsn(batch_xs[jnp.newaxis,:])\n",
    "\n",
    "d_context_grn = GatedResidualNetwork(hidden_size, key=key)\n",
    "d_context = d_context_grn(static_out[0,:])\n",
    "\n",
    "batch_xdd = synthetic_data['x_dd'][0,...]\n",
    "daily_vsn = VariableSelectionNetwork(hidden_size,daily_in_size, key=key)\n",
    "daily_out = daily_vsn(batch_xdd)\n",
    "\n",
    "daily_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471de5d-3b30-421d-881f-fca11fedf224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Selection Network\n",
    "# INIT\n",
    "keys = jax.random.split(key, 4)   \n",
    "\n",
    "transformer_keys = jax.random.split(keys[0], daily_in_size)\n",
    "variable_transformers = [\n",
    "    eqx.nn.Linear(1, hidden_size, key=k)\n",
    "    for k in transformer_keys\n",
    "]\n",
    "\n",
    "processor_keys = jax.random.split(keys[1], daily_in_size)\n",
    "variable_processors = [\n",
    "    GatedResidualNetwork(hidden_size, key=k)\n",
    "    for k in processor_keys\n",
    "]\n",
    "\n",
    "weights_size = daily_in_size*hidden_size\n",
    "weights_grn = GatedResidualNetwork(weights_size, hidden_size, key=keys[2])\n",
    "\n",
    "# CALL\n",
    "# Individual batch.\n",
    "inputs = synthetic_data['x_dd'][0,...]\n",
    "\n",
    "# Transform each variable\n",
    "transformed_inputs = jnp.stack([\n",
    "    jax.vmap(processor)(inputs[:, i][:,jnp.newaxis])\n",
    "    for i, processor in enumerate(variable_transformers)\n",
    "], axis=1)\n",
    "\n",
    "# Process each variable\n",
    "processed_inputs = jnp.stack([\n",
    "    jax.vmap(processor)(transformed_inputs[:, i], None)\n",
    "    for i, processor in enumerate(variable_processors)\n",
    "], axis=1)\n",
    "\n",
    "flattened = transformed_inputs.reshape([seq_length,-1])\n",
    "flat_weights = jax.vmap(weights_grn, in_axes=(0,None))(flattened, d_context)\n",
    "flat_weights = jax.nn.softmax(flat_weights, axis=-1)\n",
    "\n",
    "variable_weights = flat_weights.reshape(transformed_inputs.shape)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8f2f7-2d0a-49bc-93ea-d42cb2f5e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737b1b6-5886-4839-b487-995a73c0469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd7302-5ca7-4950-8a2e-189a21062767",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ed178-5ebe-477c-844c-0e146263300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a17d6b-040b-44af-b6c8-2db6c03ad200",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = flattened[0]\n",
    "\n",
    "eta2 = jax.nn.elu(eta2_dynamic(inputs) + eta2_bias)\n",
    "eta1 = eta1_linear(eta2)\n",
    "layer_norm(inputs + glu(eta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a01dd2-ad6a-4f8b-8da6-8422b9acaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn(flattened[0],None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3de82-a209-41c0-bbfa-692acded9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn = GatedResidualNetwork(daily_in_size*hidden_size, None, daily_in_size*hidden_size, daily_in_size*hidden_size, key=keys[2])\n",
    "\n",
    "flattened = transformed_inputs.reshape([seq_length,-1])\n",
    "selection_weights = jax.vmap(grn)(flattened, None)\n",
    "selection_weights = jax.nn.softmax(selection_weights, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558c769-3ffa-40ed-a364-05173238b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_weights.reshape([30,22,-1]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tss-ml]",
   "language": "python",
   "name": "conda-env-.conda-tss-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
