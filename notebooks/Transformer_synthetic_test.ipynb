{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6a101-0e8a-4298-8256-ca628c442d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from typing import Dict, List, Mapping, Optional\n",
    "\n",
    "class EmbedderBlock(eqx.Module):\n",
    "    \"\"\"\n",
    "    Embeds input data from daily and irregular time series along with position indices.\n",
    "    Includes dropout on the embeddings.\n",
    "    \"\"\"\n",
    "    data_embedder: eqx.nn.Linear\n",
    "    position_embedder: eqx.nn.Embedding\n",
    "    layernorm: eqx.nn.LayerNorm\n",
    "    dropout: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(self, \n",
    "                 dynamic_in_size: int, \n",
    "                 max_length: int, \n",
    "                 hidden_size: int,\n",
    "                 dropout_rate: float, \n",
    "                 key: jax.random.PRNGKey):\n",
    "        \n",
    "        self.data_embedder = eqx.nn.Linear(in_features=dynamic_in_size, out_features=hidden_size, key=key)\n",
    "        self.position_embedder = eqx.nn.Embedding(num_embeddings=max_length, embedding_size=hidden_size, key=key)\n",
    "        self.layernorm = eqx.nn.LayerNorm(shape=(2*hidden_size,))\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def __call__(self, \n",
    "                 dynamic_data: jnp.ndarray, \n",
    "                 position_ids: jnp.ndarray,\n",
    "                 key: jax.random.PRNGKey) -> jnp.ndarray:  \n",
    "        \n",
    "        data_embeds = jax.vmap(self.data_embedder)(dynamic_data)\n",
    "        position_embeds = jax.vmap(self.position_embedder)(position_ids)\n",
    "        \n",
    "        # embedded_inputs = daily_embeds + irregular_embeds + position_embeds\n",
    "        embedded_inputs = jnp.concatenate([data_embeds, position_embeds], axis=-1)\n",
    "        embedded_inputs = self.dropout(embedded_inputs, key=key)\n",
    "        embedded_inputs = jax.vmap(self.layernorm)(embedded_inputs)\n",
    "        return embedded_inputs\n",
    "\n",
    "\n",
    "class AttentionBlock(eqx.Module):\n",
    "    \"\"\"\n",
    "    Implements a multi-head self-attention mechanism, integrating static data into the attention process.\n",
    "    Includes dropout in the output of the attention.\n",
    "    \"\"\"\n",
    "    attention: eqx.nn.MultiheadAttention\n",
    "    layernorm: eqx.nn.LayerNorm\n",
    "    static_linear: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_size: int, \n",
    "                 num_heads: int, \n",
    "                 static_in_size: int,\n",
    "                 dropout_rate: float,\n",
    "                 key: jax.random.PRNGKey): \n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        self.attention = eqx.nn.MultiheadAttention(\n",
    "            num_heads=num_heads, \n",
    "            query_size=hidden_size, \n",
    "            key_size=hidden_size, \n",
    "            value_size=hidden_size, \n",
    "            output_size=hidden_size, \n",
    "            key=keys[0])\n",
    "        self.layernorm = eqx.nn.LayerNorm(shape=(hidden_size,))\n",
    "        self.static_linear = eqx.nn.Linear(in_features=static_in_size, out_features=hidden_size, key=keys[1])\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def __call__(self, \n",
    "                 inputs: jnp.ndarray, \n",
    "                 static_data: jnp.ndarray,\n",
    "                 base_mask: jnp.ndarray,\n",
    "                 key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "        static_embedding = self.static_linear(static_data)\n",
    "        static_embedding = jnp.expand_dims(static_embedding, axis=0)\n",
    "        modified_keys = inputs + static_embedding\n",
    "        modified_values = inputs + static_embedding\n",
    "\n",
    "        irregular_mask = jnp.tile(base_mask, (inputs.shape[0], 1))\n",
    "        daily_mask = jnp.ones_like(irregular_mask)\n",
    "        multihead_mask = jnp.stack([irregular_mask, daily_mask], axis=0)\n",
    "        \n",
    "        attention_output = self.attention(inputs, modified_keys, modified_values, multihead_mask)\n",
    "        attention_output = self.dropout(attention_output, key=key)\n",
    "        result = attention_output + inputs\n",
    "        result = jax.vmap(self.layernorm)(result)\n",
    "        return result\n",
    "\n",
    "class FeedForwardBlock(eqx.Module):\n",
    "    \"\"\"\n",
    "    Applies a two-layer feed-forward network with GELU activation in between. Includes dropout after the MLP layer.\n",
    "    \"\"\"\n",
    "    mlp: eqx.nn.Linear\n",
    "    output: eqx.nn.Linear\n",
    "    layernorm: eqx.nn.LayerNorm\n",
    "    dropout: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_size: int, \n",
    "                 intermediate_size: int,\n",
    "                 dropout_rate: float,\n",
    "                 key: jax.random.PRNGKey): \n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        self.mlp = eqx.nn.Linear(in_features=hidden_size, out_features=intermediate_size, key=keys[0])\n",
    "        self.output = eqx.nn.Linear(in_features=intermediate_size, out_features=hidden_size, key=keys[1])\n",
    "        self.layernorm = eqx.nn.LayerNorm(shape=(hidden_size,))\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def __call__(self,\n",
    "                 inputs: jnp.ndarray,\n",
    "                 key: jax.random.PRNGKey) -> jnp.ndarray: \n",
    "        hidden = self.mlp(inputs)\n",
    "        hidden = jax.nn.gelu(hidden)\n",
    "        hidden = self.dropout(hidden, key=key)\n",
    "        output = self.output(hidden)\n",
    "        output += inputs\n",
    "        output = self.layernorm(output) \n",
    "        return output\n",
    "\n",
    "class TransformerLayer(eqx.Module):\n",
    "    attention_block: AttentionBlock\n",
    "    ff_block: FeedForwardBlock\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_size: int, \n",
    "                 intermediate_size: int, \n",
    "                 num_heads: int, \n",
    "                 static_in_size: int, \n",
    "                 dropout_p: float,\n",
    "                 key: jax.random.PRNGKey):\n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        self.attention_block = AttentionBlock(hidden_size, num_heads, static_in_size, dropout_p, keys[0])\n",
    "        self.ff_block = FeedForwardBlock(hidden_size, intermediate_size, dropout_p, keys[1])\n",
    "\n",
    "    def __call__(self, \n",
    "                 inputs: jnp.ndarray, \n",
    "                 static_data: jnp.ndarray,\n",
    "                 mask: jnp.ndarray,\n",
    "                 key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        attention_output = self.attention_block(inputs, static_data, mask, keys[0])\n",
    "        \n",
    "        ff_keys = jax.random.split(keys[1], attention_output.shape[0])\n",
    "        output = jax.vmap(self.ff_block)(attention_output, ff_keys)\n",
    "        return output\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    embedder_block: EmbedderBlock\n",
    "    layers: List[TransformerLayer]\n",
    "    pooler: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, \n",
    "                 dynamic_in_size: int, \n",
    "                 static_in_size: int, \n",
    "                 max_length: int, \n",
    "                 hidden_size: int, \n",
    "                 intermediate_size: int, \n",
    "                 num_layers: int, \n",
    "                 num_heads: int, \n",
    "                 dropout_p: float,\n",
    "                 key: jax.random.PRNGKey): \n",
    "        keys = jax.random.split(key, num=3)\n",
    "        \n",
    "        self.embedder_block = EmbedderBlock(dynamic_in_size, max_length, hidden_size, dropout_p, keys[0])\n",
    "        layer_keys = jax.random.split(keys[1], num=num_layers)\n",
    "        self.layers = [TransformerLayer(2*hidden_size, intermediate_size, num_heads, static_in_size, dropout_p, layer_key) for layer_key in layer_keys]\n",
    "        self.pooler = eqx.nn.Linear(in_features=2*hidden_size, out_features=hidden_size, key=keys[2])\n",
    "\n",
    "    def __call__(self, \n",
    "                 data: dict, \n",
    "                 position_ids: jnp.ndarray, \n",
    "                 key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        embeddings = self.embedder_block(data['x_d'], position_ids, keys[0])\n",
    "\n",
    "        x = embeddings\n",
    "        layer_keys = jax.random.split(keys[1], len(self.layers))\n",
    "        for layer, layer_key in zip(self.layers, layer_keys):\n",
    "            x = layer(x, data['x_s'], data['mask'], layer_key)\n",
    "        first_token_last_layer = x[..., 0, :]\n",
    "        pooled = self.pooler(first_token_last_layer)\n",
    "        pooled = jnp.tanh(pooled)\n",
    "        return pooled\n",
    "\n",
    "class EATransformer(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    head: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, \n",
    "                 dynamic_in_size: int, \n",
    "                 static_in_size: int, \n",
    "                 max_length: int, \n",
    "                 hidden_size: int, \n",
    "                 intermediate_size: int, \n",
    "                 num_layers: int, \n",
    "                 num_heads: int, \n",
    "                 out_size: int,\n",
    "                 dropout_p: float, \n",
    "                 seed: int):\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        keys = jax.random.split(key)\n",
    "        \n",
    "        self.encoder = Encoder(dynamic_in_size=dynamic_in_size,\n",
    "                               static_in_size=static_in_size,\n",
    "                               max_length=max_length,\n",
    "                               hidden_size=hidden_size,\n",
    "                               intermediate_size=intermediate_size,\n",
    "                               num_layers=num_layers,\n",
    "                               num_heads=num_heads,\n",
    "                               dropout_p=dropout_p,\n",
    "                               key=keys[0])\n",
    "        self.head = eqx.nn.Linear(in_features=hidden_size, out_features=out_size, key=keys[1])\n",
    "\n",
    "    def __call__(self, data: dict, key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "        # Kluge to get it running now. Will address data loader later.\n",
    "        position_ids = jnp.arange(data['x_d'].shape[0]).astype(jnp.int32)\n",
    "\n",
    "        # for key, value in data.items():\n",
    "        #     print(f\"{key}: {value.shape}\")\n",
    "        # Kluge to get it running now. Will address data loader later.\n",
    "        \n",
    "        pooled_output = self.encoder(data, position_ids, key)\n",
    "        return self.head(pooled_output)\n",
    "\n",
    "# Configuration\n",
    "daily_size = 1\n",
    "irregular_size = 6\n",
    "static_in_size = 31\n",
    "num_samples = 10000\n",
    "sequence_length = 30\n",
    "p_missing_data = 0.3\n",
    "\n",
    "config = {\n",
    "    \"dynamic_in_size\": daily_size + irregular_size,\n",
    "    \"static_in_size\": static_in_size,          \n",
    "    \"max_length\": 30, \n",
    "    \"hidden_size\": 256,\n",
    "    \"intermediate_size\": 1024,\n",
    "    \"num_layers\": 8,\n",
    "    \"num_heads\": 2,\n",
    "    \"out_size\": 1,\n",
    "    \"dropout_p\": 0.4,\n",
    "    \"seed\": 0\n",
    "}\n",
    "model = EATransformer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e52e69-fbf1-498e-a33d-6d458f1b2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "params = eqx.filter(model, eqx.is_inexact_array)\n",
    "leaves = jax.tree_util.tree_leaves(params)\n",
    "n_params = np.sum([l.size for l in leaves])\n",
    "\n",
    "def human_readable_size(size_bytes):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "\n",
    "print(human_readable_size(n_params*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9d006-fb00-4586-9c0c-4c61c951122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "daily_data = np.random.rand(num_samples, daily_size).astype(np.float32)\n",
    "irregular_data = np.random.rand(num_samples, irregular_size).astype(np.float32)\n",
    "static_data = np.random.rand(num_samples, static_in_size).astype(np.float32)\n",
    "targets = (\n",
    "    0.5 * np.sum(daily_data, axis=1) +\n",
    "    0.3 * np.sum(irregular_data, axis=1) +\n",
    "    0.2 * np.sum(static_data, axis=1)# +\n",
    "    # np.random.normal(0, 0.01, num_samples)  # Add some noise\n",
    ").reshape(num_samples, 1).astype(np.float32)\n",
    "\n",
    "# Now delete some data from irregular\n",
    "valid_mask = np.random.rand(num_samples) > p_missing_data   \n",
    "irregular_data[~valid_mask,:] = 0\n",
    "\n",
    "# Convert to JAX arrays\n",
    "daily_data = jnp.array(daily_data)\n",
    "irregular_data = jnp.array(irregular_data)\n",
    "static_data = jnp.array(static_data)\n",
    "targets = jnp.array(targets)\n",
    "\n",
    "print(\"Hydrology data shape:\", daily_data.shape)\n",
    "print(\"Satellite data shape:\", irregular_data.shape)\n",
    "print(\"Static data shape:\", static_data.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7eb642-0f17-4675-b58e-818686a9272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import tqdm\n",
    "\n",
    "# Define loss function\n",
    "def mse_loss(predictions, targets):\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Define training step\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, batch, batch_keys):\n",
    "    predictions = jax.vmap(model)(batch, batch_keys)\n",
    "    return mse_loss(predictions, batch['y'])\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, batch, batch_keys):\n",
    "    loss, grads = compute_loss(model, batch, batch_keys)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_batches = num_samples // batch_size\n",
    "learning_rate = 1e-3\n",
    "epochs = 25\n",
    "\n",
    "# Create an optimizer\n",
    "tx = optax.adam(learning_rate)\n",
    "opt_state = tx.init(model)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    sample_perm = np.random.permutation(range(sequence_length, num_samples))\n",
    "    # Check if the number of elements matches num_batches * batch_size\n",
    "    assert len(sample_perm)%batch_size == 0, \"Number of samples must evenly divide into batch size\"\n",
    "    batched_samples = sample_perm.reshape((-1, batch_size))\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm.tqdm(batched_samples,desc=f\"Epoch {epoch+1}\")\n",
    "    for i, sample_ids in enumerate(pbar):\n",
    "        batch_ids = []\n",
    "        for sample_id in sample_ids:\n",
    "            seq_start = sample_id - sequence_length\n",
    "            seq_end = sample_id\n",
    "            batch_ids.append((seq_start, seq_end))\n",
    "        batch_ids = jnp.array([np.arange(seq_start, seq_end) for seq_start, seq_end in batch_ids])\n",
    "            \n",
    "        x_d = jnp.concat([daily_data[batch_ids], irregular_data[batch_ids]], axis=-1)\n",
    "        batch = {\"x_d\": x_d,\n",
    "                 \"mask\": valid_mask[batch_ids],\n",
    "                 \"x_s\": static_data[batch_ids[:,0]],\n",
    "                 \"y\": targets[batch_ids[:,-1]]}\n",
    "        keys = jax.random.split(key, batch_size+1)\n",
    "        key = keys[0]\n",
    "        batch_keys = keys[1:]\n",
    "\n",
    "        model, opt_state, loss = train_step(model, opt_state, batch, batch_keys)\n",
    "        epoch_loss += loss\n",
    "        avg_loss = epoch_loss / (i+1)\n",
    "        pbar.set_postfix_str(f\"Loss: {avg_loss:.4f}\")\n",
    "    loss_list.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb19465-6517-4371-b894-32f8f8060402",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mask = batch['mask'][0]\n",
    "\n",
    "irregular_mask = jnp.tile(base_mask, (sequence_length, 1))\n",
    "daily_mask = jnp.ones_like(irregular_mask)\n",
    "\n",
    "multihead_mask = jnp.stack([irregular_mask, daily_mask], axis=0)\n",
    "\n",
    "plt.imshow(multihead_mask[1], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03399494-8663-4d34-8568-f4470653cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a36d127b-eb4c-4277-8370-7d8fceae1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from functools import partial\n",
    "from typing import cast, Optional, Union\n",
    "from jaxtyping import Array, Bool, Float, PRNGKeyArray\n",
    "\n",
    "def biased_dot_product_attention_weights(\n",
    "    query: Float[Array, \"q_seq qk_size\"],\n",
    "    key: Float[Array, \"kv_seq qk_size\"],\n",
    "    bias: Float[Array, \"q_seq kv_seq\"], \n",
    "    mask: Optional[Bool[Array, \"q_seq kv_seq\"]] = None) -> Float[Array, \"q_seq kv_seq\"]:\n",
    "    \n",
    "    query = query / math.sqrt(query.shape[-1])\n",
    "    logits = jnp.einsum(\"sd,Sd->sS\", query, key) + bias\n",
    "\n",
    "    if mask is not None:\n",
    "        if mask.shape != logits.shape:\n",
    "            raise ValueError(\n",
    "                f\"mask must have shape (query_seq_length, \"\n",
    "                f\"kv_seq_length)=({query.shape[0]}, \"\n",
    "                f\"{key.shape[0]}). Got {mask.shape}.\"\n",
    "            )\n",
    "        logits = jnp.where(mask, logits, jnp.finfo(logits.dtype).min)\n",
    "        logits = cast(Array, logits)\n",
    "\n",
    "    with jax.numpy_dtype_promotion(\"standard\"):\n",
    "        dtype = jnp.result_type(logits.dtype, jnp.float32)\n",
    "    weights = jax.nn.softmax(logits.astype(dtype)).astype(logits.dtype)\n",
    "    return weights\n",
    "\n",
    "def dot_product_attention(\n",
    "    query: Float[Array, \"q_seq qk_size\"],\n",
    "    key_: Float[Array, \"kv_seq qk_size\"],\n",
    "    value: Float[Array, \"kv_seq v_size\"],\n",
    "    bias: Float[Array, \"q_seq kv_seq\"], \n",
    "    mask: Optional[Bool[Array, \"q_seq kv_seq\"]] = None,\n",
    "    dropout: Optional[eqx.nn.Dropout] = None,\n",
    "    *,\n",
    "    key: Optional[PRNGKeyArray] = None,\n",
    "    inference: Optional[bool] = None) -> Float[Array, \"q_seq v_size\"]:\n",
    "    \n",
    "    weights = biased_dot_product_attention_weights(query, key_, bias, mask)\n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights, key=key, inference=inference)\n",
    "    attn = jnp.einsum(\"sS,Sd->sd\", weights, value)\n",
    "    return attn\n",
    "\n",
    "class LogitBiasedMultiheadAttention(eqx.nn.MultiheadAttention):\n",
    "    def __call__(self, query, key_, value, logit_bias, mask=None, *, key=None, inference=None, deterministic=None, process_heads=None):\n",
    "        \"\"\"Extends the call method to pass the bias term through to the attention computation.\"\"\"\n",
    "        query_seq_length, _ = query.shape\n",
    "        kv_seq_length, _ = key_.shape\n",
    "        kv_seq_length2, _ = value.shape\n",
    "        if kv_seq_length != kv_seq_length2:\n",
    "            # query length can be different\n",
    "            raise ValueError(\"key and value must both be sequences of equal length.\")\n",
    "\n",
    "        query_heads = self._project(self.query_proj, query)\n",
    "        key_heads = self._project(self.key_proj, key_)\n",
    "        value_heads = self._project(self.value_proj, value)\n",
    "        logit_bias_heads = logit_bias.reshape(logit_bias.shape[0], self.num_heads, -1)\n",
    "\n",
    "        # Removed option for now.\n",
    "        \"\"\"\n",
    "        if process_heads is not None:\n",
    "            q_shape, k_shape, v_shape, b_shape = (query_heads.shape,\n",
    "                                                  key_heads.shape,\n",
    "                                                  value_heads.shape,\n",
    "                                                  logit_bias_heads.shape)\n",
    "            \n",
    "            heads = process_heads(query_heads, key_heads, value_heads, logit_bias_heads)\n",
    "            query_heads, key_heads, value_heads, logit_bias_heads = heads # unpack\n",
    "\n",
    "            shape_changed = (query_heads.shape != q_shape\n",
    "                             or key_heads.shape != k_shape\n",
    "                             or value_heads.shape != v_shape\n",
    "                             or logit_bias_heads.shape != b_shape)\n",
    "            if (shape_changed):\n",
    "                raise ValueError(\"process_heads must not change the shape of the heads.\")\n",
    "        \"\"\"\n",
    "        \n",
    "        attn_fn = partial(dot_product_attention, dropout=self.dropout, inference=inference)\n",
    "        \n",
    "        keys = None if key is None else jax.random.split(key, query_heads.shape[1])\n",
    "        if mask is not None and mask.ndim == 3:\n",
    "            # Batch `mask` and `keys` down their 0-th dimension.\n",
    "            attn = jax.vmap(attn_fn, in_axes=1, out_axes=1)(\n",
    "                query_heads, key_heads, value_heads, logit_bias_heads, mask=mask, key=keys\n",
    "            )\n",
    "        else:\n",
    "            # Batch `keys` down its 0-th dimension.\n",
    "            attn = jax.vmap(partial(attn_fn, mask=mask), in_axes=1, out_axes=1)(\n",
    "                query_heads, key_heads, value_heads, logit_bias_heads, key=keys\n",
    "            )\n",
    "        attn = attn.reshape(query_seq_length, -1)\n",
    "\n",
    "        return jax.vmap(self.output_proj)(attn)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_heads = 4\n",
    "model_dim = 32  # Ensure this is divisible by num_heads for simplicity\n",
    "seq_length = 10\n",
    "key_dim = model_dim\n",
    "value_dim = model_dim\n",
    "\n",
    "# Create synthetic data\n",
    "key, subkey = jax.random.split(key)\n",
    "queries = jax.random.normal(subkey, (seq_length, model_dim))\n",
    "keys = jax.random.normal(subkey, (seq_length, model_dim))\n",
    "values = jax.random.normal(subkey, (seq_length, model_dim))\n",
    "biases = jax.random.normal(subkey, (seq_length, seq_length*num_heads)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1364c391-e2ab-4d7f-8138-1b4780552cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ef565f5-85e4-408a-9aa7-bd78f3abb94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  weight=f32[4,32],\n",
       "  bias=f32[4],\n",
       "  in_features=32,\n",
       "  out_features=4,\n",
       "  use_bias=True\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logit_bias_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4302bf06-c59c-493e-b277-7c5aed5fea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 40)\n",
      "(10, 4, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x169b476d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the module\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = LogitBiasedMultiheadAttention(\n",
    "    num_heads=num_heads,\n",
    "    query_size=model_dim,\n",
    "    key_size=key_dim,\n",
    "    value_size=value_dim,\n",
    "    output_size=model_dim,\n",
    "    dropout_p=0.1,\n",
    "    inference=False,\n",
    "    key=key\n",
    ")\n",
    "\n",
    "# Call the attention module\n",
    "result = model(queries, keys, values, biases, key=key)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23767f8d-5826-4598-a6d6-ae7237bac9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  weight=f32[32,32],\n",
       "  bias=f32[32],\n",
       "  in_features=32,\n",
       "  out_features=32,\n",
       "  use_bias=True\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logit_bias_proj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
