{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9667724b-6616-4d46-8658-7d7def281f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from shapely import wkt\n",
    "import ast\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "datasets = Path(\"/nas/cee-water/cjgleason/data\")\n",
    "era5_dir = datasets / \"ERA5-Land/sub_basin_timeseries\"\n",
    "swot_lake_dir = datasets / 'hydrocron' / 'lake'\n",
    "\n",
    "save_dir = Path(\"/nas/cee-water/cjgleason/ted/swot-ml/data/distributed\")\n",
    "preprocess_dir = save_dir / \"preprocess\"\n",
    "metadata_dir = save_dir / \"metadata\"\n",
    "\n",
    "ts_dir = save_dir / \"time_series\"\n",
    "ts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# basin_name = 'Ohio'\n",
    "basin_name = 'Upper_Miss'\n",
    "\n",
    "\n",
    "matchups = gpd.read_file(metadata_dir / f'{basin_name}_matchups.geojson').set_index(\"HYBAS_ID\")\n",
    "matchups.index = matchups.index.astype(str)\n",
    "\n",
    "\n",
    "# Safely convert stringified lists/dicts back to Python objects\n",
    "def safe_literal_eval(df, col):\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n",
    "    return df\n",
    "\n",
    "for col in [\"mb_values\", \"lake_reach_ids\", \"lake_pld_ids\"]:\n",
    "    matchups = safe_literal_eval(matchups, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd48913-77f3-46e9-9c34-162f30e337ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: USGS database is 60 days old. Consider updating.\n"
     ]
    }
   ],
   "source": [
    "import global_gauges as gg\n",
    "\n",
    "facade = gg.GaugeDataFacade(providers='usgs')\n",
    "sites = facade.get_stations_n_days(90)\n",
    "\n",
    "site_ids = matchups[~matchups['site_id'].isna()]['site_id'].unique()\n",
    "sites = sites.loc[site_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e85e7e-4768-4431-bc75-08385d47cbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3be20c-cd92-49ca-a26b-a3ea96b5b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import itertools \n",
    "\n",
    "lake_ids = list(itertools.chain.from_iterable(matchups['lake_reach_ids']))\n",
    "river_reaches = matchups['reach_id'].dropna().astype(int).to_list()\n",
    "all_reaches = lake_ids + river_reaches\n",
    "\n",
    "fields=[\n",
    "    'reach_id', 'time','wse', 'wse_u', 'wse_r_u',\n",
    "    'slope', 'slope_u', 'slope_r_u', 'slope2', 'slope2_u', 'slope2_r_u',\n",
    "    'width', 'width_u',\n",
    "    'area_total', 'area_tot_u', 'area_detct', 'area_det_u', 'area_wse',\n",
    "    'layovr_val', 'node_dist', 'loc_offset', 'xtrk_dist',\n",
    "    'reach_q', 'reach_q_b', 'dark_frac', 'ice_clim_f', 'partial_f',\n",
    "    'obs_frac_n', 'xovr_cal_q', 'dry_trop_c', 'wet_trop_c', 'iono_c', 'xovr_cal_c'\n",
    "]\n",
    "\n",
    "continents = ['na']\n",
    "swot = []\n",
    "for con in continents:\n",
    "    con_file = datasets / 'hydrocron' / 'reach' / (con + '_hydrocron_reach.parquet')\n",
    "    dataset = ds.dataset(con_file, format=\"parquet\")\n",
    "    table = dataset.to_table(\n",
    "        columns=fields,\n",
    "        filter=(ds.field(\"reach_id\").isin(all_reaches))\n",
    "    )\n",
    "    swot.append(table.to_pandas())\n",
    "    \n",
    "all_swot = pd.concat(swot)\n",
    "all_swot = all_swot[all_swot['wse'] != -999999999999.0]\n",
    "all_swot['d_wse'] = all_swot['wse'] - all_swot.groupby('reach_id')['wse'].transform('median')\n",
    "all_swot['d_width'] = all_swot['width'] - all_swot.groupby('reach_id')['width'].transform('median')\n",
    "\n",
    "all_swot = all_swot.set_index(['reach_id','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c0a7dd-df46-406b-a16c-27874c6f34e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMID</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">71000001</th>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>854.720882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>1.663859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>1075.322720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-23</th>\n",
       "      <td>1527.924744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-11</th>\n",
       "      <td>1645.340463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">78028470</th>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>115.782824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>540.976377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29</th>\n",
       "      <td>251.693162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-21</th>\n",
       "      <td>304.667251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31</th>\n",
       "      <td>59.155982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14361294 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           width\n",
       "COMID    date                   \n",
       "71000001 2019-01-15   854.720882\n",
       "         2019-02-04     1.663859\n",
       "         2019-02-09  1075.322720\n",
       "         2019-05-23  1527.924744\n",
       "         2019-06-11  1645.340463\n",
       "...                          ...\n",
       "78028470 2020-06-22   115.782824\n",
       "         2020-12-24   540.976377\n",
       "         2021-03-29   251.693162\n",
       "         2022-08-21   304.667251\n",
       "         2022-08-31    59.155982\n",
       "\n",
       "[14361294 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glow_dir = datasets / \"GLOW-S\" / \"daily_reach_aggregated\"\n",
    "\n",
    "glow = pd.read_parquet(glow_dir / \"region_7_daily_median.parquet\")\n",
    "glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f31b6c-8ec0-49e8-8855-dc15e20eac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched reach_ids: 1712\n",
      "Number of unprocessed basins: 1712\n",
      "Number of unprocessed basins with SWOT data: 1712\n",
      "Number of unprocessed basins with era5 file: 1712\n"
     ]
    }
   ],
   "source": [
    "# Filter out reach_ids that already have a processed file\n",
    "def get_reaches_to_process():\n",
    "    to_process = matchups.copy()\n",
    "    print(f\"Total matched reach_ids: {len(to_process.index)}\")\n",
    "\n",
    "    processed_stems = [f.stem for f in list(ts_dir.glob('*.nc'))]\n",
    "    processed_mask = to_process.index.astype(str).isin(processed_stems)\n",
    "    to_process = to_process[~processed_mask]\n",
    "    print(f\"Number of unprocessed basins: {len(to_process.index)}\")\n",
    "    \n",
    "    all_swot_ids = all_swot.index.get_level_values('reach_id').unique()\n",
    "    processed_mask = to_process.index.isin(all_swot_ids)\n",
    "    to_process = to_process[~processed_mask]\n",
    "    print(f\"Number of unprocessed basins with SWOT data: {len(to_process.index)}\")\n",
    "    \n",
    "    era5_stems = [p.stem for p in (era5_dir / 'basin_timeseries').glob('*.parquet')]\n",
    "    processed_mask = to_process.index.astype(str).isin(era5_stems)\n",
    "    to_process = to_process[~processed_mask]\n",
    "    print(f\"Number of unprocessed basins with era5 file: {len(to_process.index)}\")\n",
    "    \n",
    "    return to_process.index.unique()\n",
    "\n",
    "to_process = get_reaches_to_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a90629-fd3c-43a3-a61f-c751ca760fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"1980-01-01\"\n",
    "t2 = \"2024-12-31\"\n",
    "\n",
    "pld_fields = [\n",
    "    \"n_overlap\", \"wse\", \"wse_u\", \"wse_r_u\", \"wse_std\",\n",
    "    \"area_total\", \"area_tot_u\", \"area_detct\", \"area_det_u\",\n",
    "    \"dark_frac\", \"xovr_cal_q\",  \"layovr_val\", \"xtrk_dist\",\n",
    "    \"quality_f\",  \"partial_f\", \"ice_clim_f\",\n",
    "    \"dry_trop_c\", \"wet_trop_c\", \"iono_c\", \"xovr_cal_c\"\n",
    "]\n",
    "\n",
    "def get_glow_s(merit_reaches):\n",
    "    glow_mask = glow.index.get_level_values('COMID').isin(merit_reaches)\n",
    "    if glow_mask.any():\n",
    "        glow_ix = glow[glow_mask].groupby('date').median()\n",
    "        glow_ix.index = pd.to_datetime(glow_ix.index).tz_localize('UTC')\n",
    "        return glow_ix.rename(columns={'width':'glow_width'})\n",
    "    return pd.DataFrame(columns = ['glow_width'])\n",
    "\n",
    "\n",
    "def get_swot_r(reach_id):\n",
    "    if reach_id in all_swot.index.get_level_values('reach_id'):\n",
    "        swot = all_swot.xs(reach_id, level='reach_id')\n",
    "        swot.index = pd.to_datetime(swot.index).tz_localize('UTC')\n",
    "        return swot\n",
    "    return pd.DataFrame(columns = all_swot.columns)\n",
    "\n",
    "def get_swot_l(pld_ids: list):\n",
    "    lake_dfs = []\n",
    "    for pld_id in pld_ids:\n",
    "        path = Path(swot_lake_dir / f\"{pld_id}.parquet\")\n",
    "        if path.is_file():\n",
    "            swot = pd.read_parquet(path)[pld_fields]\n",
    "            swot = swot.replace(-999999999999.0, np.nan)\n",
    "            swot.dropna(subset=['wse', 'area_total'])\n",
    "    \n",
    "            wse = swot['wse']\n",
    "            area = swot['area_total']\n",
    "            swot['d_wse'] = wse - wse.median()\n",
    "            swot['d_area'] = area - area.median()\n",
    "            swot['d_volume'] = swot['d_wse'] * (0.5*(area + area.median()))\n",
    "            lake_dfs.append(swot)\n",
    "        else:\n",
    "            lake_dfs.append([])\n",
    "\n",
    "    df_lens = [len(d) for d in lake_dfs]\n",
    "    if any(l>0 for l in df_lens):\n",
    "        swot = lake_dfs[np.argmax(df_lens)]\n",
    "        swot.index = swot.index.normalize().tz_convert('UTC')\n",
    "        return swot\n",
    "    else:\n",
    "        new_fields = ['d_wse', 'd_area', 'd_volume']\n",
    "        return pd.DataFrame(columns = pld_fields + new_fields)\n",
    "    \n",
    "\n",
    "def get_gauge(site_id):\n",
    "    if site_id is not None:\n",
    "        gauge = facade.get_daily_values(site_id, t1, t2).droplevel('site_id')\n",
    "        gauge.index = gauge.index.tz_localize('UTC')\n",
    "        return gauge[['discharge']]\n",
    "    return pd.DataFrame(columns = ['discharge'])\n",
    "\n",
    "\n",
    "def get_era5(reach_id):\n",
    "    path = Path(era5_dir / f\"{reach_id}.parquet\")\n",
    "    \n",
    "    era5 = pd.read_parquet(path)\n",
    "    era5.index = era5.index.tz_localize('UTC')\n",
    "    era5.index.name = 'datetime'\n",
    "    era5.fillna(0, inplace=True)\n",
    "    return era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612f2d28-4aca-471b-a8b8-a74f72498f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched reach_ids: 1712\n",
      "Number of unprocessed basins: 1712\n",
      "Number of unprocessed basins with SWOT data: 1712\n",
      "Number of unprocessed basins with era5 file: 1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1712/1712 [12:49<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "ts_dir.mkdir(exist_ok=True)\n",
    "date_range = pd.date_range(start=t1, end=t2, freq='D', tz='UTC')\n",
    "to_process = get_reaches_to_process()\n",
    "# to_process = matchups.index.unique()\n",
    "\n",
    "for hybas_id in tqdm(to_process, total=len(to_process), desc=\"Writing files\"):\n",
    "    nc_file_path = ts_dir / f\"{hybas_id}.nc\"\n",
    "    \n",
    "    matchups_ix = matchups.loc[hybas_id]\n",
    "\n",
    "  \n",
    "    era5_df = get_era5(hybas_id)\n",
    "    swot_r_df = get_swot_r(matchups_ix['reach_id']).add_suffix('_reach')\n",
    "    swot_l_df = get_swot_l(matchups_ix['lake_pld_ids']).add_suffix('_lake')\n",
    "    glow_df = get_glow_s(matchups_ix['mb_values'])\n",
    "    gauge_df = get_gauge(matchups_ix['site_id'])\n",
    "         \n",
    "    # Merge all of the filtered datasets together.\n",
    "    dataframes = [era5_df, swot_r_df, swot_l_df, glow_df, gauge_df]\n",
    "    df = pd.DataFrame(index=date_range)\n",
    "    for dataframe in dataframes:\n",
    "        df = df.join(dataframe, how='left')\n",
    "    \n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    df.index.name = 'date'\n",
    "    \n",
    "    if pd.infer_freq(df.index) !=  \"D\":\n",
    "        raise RuntimeError(\"Non-daily time freq found\")\n",
    "\n",
    "    # Save netcdf\n",
    "    dates = df.index.values\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(date=(dates))\n",
    "    ds.to_netcdf(nc_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cd21b-2854-441f-80d7-44ea0fe6d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(glow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01e3a-4019-4399-9a86-8eff934b042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.index, df['d_volume_lake'])\n",
    "plt.scatter(df.index, df['d_wse_reach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc541b-8636-4384-8e35-54aa7f0f573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.index, df['discharge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66dd77-f5c1-4425-9678-14099e9da08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baef4e-84de-4b84-9d5a-62f30126b488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074f709-b055-476b-bd8a-c0ac98f598fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm(ts_dir.glob(\"*.nc\")):\n",
    "    tmp_ds = xr.open_dataset(p)\n",
    "    tmp_df = tmp_ds.to_dataframe()\n",
    "    if (~tmp_df['glow_width'].isna()).sum() > 0:\n",
    "        break\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cda680-0390-4a32-8047-c2969368f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tmp_df.index, tmp_df['glow_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50c3f2-d5ef-4cf7-bb8d-8e87d3d77715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5828-4fad-4c96-b2a0-fc6b0ee6221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1bb49-c262-4932-b1f3-a05b47b40b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = xr.open_dataset(next(ts_dir.glob('*.nc')))\n",
    "list(sample.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135db6c-c9d2-45b8-b24d-b3c5f2c5862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in ts_dir.glob('*.nc'):\n",
    "    df = xr.open_dataset(fp).to_dataframe()\n",
    "    \n",
    "    if df['discharge'].isna().mean()<1:\n",
    "        break\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b7077-df33-4992-9bca-5a57345887ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.close('all')\n",
    "# df['discharge'].plot()\n",
    "# plt.errorbar(df['discharge'], df['d_wse'], yerr=df['wse_u'], fmt=\"o\")\n",
    "plt.scatter(df['discharge'], df['d_wse'], c=df['wse_u'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tss-ml]",
   "language": "python",
   "name": "conda-env-.conda-tss-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
