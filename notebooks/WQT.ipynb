{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380a658c-3cea-462e-98de-cdadad7f3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "src = str(Path('../src').resolve())\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)\n",
    "\n",
    "import models\n",
    "reload(models)\n",
    "\n",
    "cfg = {'model': 'fusiontransformer',\n",
    "       'model_args':{\n",
    "           'daily_in_size': 8,\n",
    "           'irregular_in_size': 12,\n",
    "           'static_in_size': 38,\n",
    "           'seq_length': 30,\n",
    "           'hidden_size': 64,\n",
    "           'intermediate_size': 64,\n",
    "           'num_layers': 3,\n",
    "           'num_heads': 3,\n",
    "           'out_size': 2,\n",
    "           'dropout': 0.1,\n",
    "           'seed': 0\n",
    "           }\n",
    "      }\n",
    "           \n",
    "model = models.make(cfg)\n",
    "\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cce23e50-39c2-46a3-9ace-e70be8cf426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era5: (32,)\n",
      "landsat: (32,)\n",
      "(64,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`LayerNorm(shape)(x)` must satisfy the invariant `shape == x.shape`Received `shape=(32,) and `x.shape=()`. You might need to replace `layer_norm(x)` with `jax.vmap(layer_norm)(x)`.\n\nIf this is a new error for you, it might be because this became stricter in Equinox v0.11.0. Previously all that was required is that `x.shape` ended with `shape`. However, this turned out to be a frequent source of bugs, so we made the check stricter!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key,batch_size)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[204], line 275\u001b[0m, in \u001b[0;36mHydroTransformer.__call__\u001b[0;34m(self, data, days, static, masks, key)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    267\u001b[0m              data: Dict[\u001b[38;5;28mstr\u001b[39m, jnp\u001b[38;5;241m.\u001b[39mndarray],  \u001b[38;5;66;03m# Dict of (seq_len, features) arrays\u001b[39;00m\n\u001b[1;32m    268\u001b[0m              days: Dict[\u001b[38;5;28mstr\u001b[39m, jnp\u001b[38;5;241m.\u001b[39mndarray],  \u001b[38;5;66;03m# Dict of (seq_len,) arrays\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# Encode input data\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Learned pooling of encoded data\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoded), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[204], line 234\u001b[0m, in \u001b[0;36mHierarchicalEncoder.__call__\u001b[0;34m(self, data, days, static, masks, key)\u001b[0m\n\u001b[1;32m    232\u001b[0m cross_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(keys[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_stream_layers))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, layer_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_stream_layers, cross_keys):\n\u001b[0;32m--> 234\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined\n",
      "Cell \u001b[0;32mIn[204], line 152\u001b[0m, in \u001b[0;36mTransformerLayer.__call__\u001b[0;34m(self, x, static, mask, key)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m              x: jnp\u001b[38;5;241m.\u001b[39mndarray,  \u001b[38;5;66;03m# Shape: (seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m              static: jnp\u001b[38;5;241m.\u001b[39mndarray,  \u001b[38;5;66;03m# Shape: (d_model,)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m              mask: Optional[jnp\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m              key: Optional[jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m              ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:  \u001b[38;5;66;03m# Shape: (seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     attn_key, ff_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key) \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 152\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x), static, mask, attn_key)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_block)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/equinox/nn/_normalisation.py:130\u001b[0m, in \u001b[0;36mLayerNorm.__call__\u001b[0;34m(self, x, state, key)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"**Arguments:**\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m- `x`: A JAX array, with the same shape as the `shape` passed to `__init__`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mreturned.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LayerNorm(shape)(x)` must satisfy the invariant `shape == x.shape`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived `shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and `x.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. You might need \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto replace `layer_norm(x)` with `jax.vmap(layer_norm)(x)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this is a new error for you, it might be because this became \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstricter in Equinox v0.11.0. Previously all that was required is that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x.shape` ended with `shape`. However, this turned out to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequent source of bugs, so we made the check stricter!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m mean \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(x, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    141\u001b[0m variance \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mvar(x, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: `LayerNorm(shape)(x)` must satisfy the invariant `shape == x.shape`Received `shape=(32,) and `x.shape=()`. You might need to replace `layer_norm(x)` with `jax.vmap(layer_norm)(x)`.\n\nIf this is a new error for you, it might be because this became stricter in Equinox v0.11.0. Previously all that was required is that `x.shape` ended with `shape`. However, this turned out to be a frequent source of bugs, so we made the check stricter!"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key,batch_size)\n",
    "\n",
    "jax.vmap(model)(data, days, static, masks, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476cac3-e3df-4cb9-b4c3-fa0bd7ccf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.cross_stream_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "42698dcc-571e-4ef2-ab95-c02ff75b2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 40\n",
    "\n",
    "data = {\n",
    "    \"era5\": jax.random.normal(data_key, (batch_size, seq_len, dynamic_sizes[\"era5\"])),\n",
    "    \"landsat\": jax.random.normal(data_key, (batch_size, seq_len, dynamic_sizes[\"landsat\"]))\n",
    "}\n",
    "days = {\n",
    "    \"era5\": jnp.tile(jnp.arange(seq_len),[batch_size,1]),\n",
    "    \"landsat\": jnp.tile(jax.random.randint(data_key, (seq_len,), 0, 365),[batch_size,1])\n",
    "}\n",
    "static = jax.random.normal(data_key, (batch_size, static_size))\n",
    "masks = {\n",
    "    \"era5\": jnp.ones((batch_size, seq_len), dtype=bool),\n",
    "    \"landsat\": jax.random.bernoulli(data_key, 0.8, (batch_size, seq_len))\n",
    "}\n",
    "targets = jax.random.normal(data_key, (batch_size, seq_len, output_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bbf46-dbd3-428c-ae47-b2fe15d46eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e05498-e25f-4d45-ac38-9cea41c4c9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a076ab7-95fa-4345-a7de-f1657a24943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TransformerLayer(\n",
       "   attention=ModulatedAttention(\n",
       "     mha=MultiheadAttention(\n",
       "       query_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       key_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       value_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       output_proj=Linear(\n",
       "         weight=f32[64,60],\n",
       "         bias=None,\n",
       "         in_features=60,\n",
       "         out_features=64,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       dropout=Dropout(p=0.1, inference=False),\n",
       "       num_heads=6,\n",
       "       query_size=64,\n",
       "       key_size=64,\n",
       "       value_size=64,\n",
       "       output_size=64,\n",
       "       qk_size=10,\n",
       "       vo_size=10,\n",
       "       use_query_bias=False,\n",
       "       use_key_bias=False,\n",
       "       use_value_bias=False,\n",
       "       use_output_bias=False\n",
       "     ),\n",
       "     static_proj=Linear(\n",
       "       weight=f32[64,64],\n",
       "       bias=f32[64],\n",
       "       in_features=64,\n",
       "       out_features=64,\n",
       "       use_bias=True\n",
       "     )\n",
       "   ),\n",
       "   ff_block=MLP(\n",
       "     layers=(\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[128,64],\n",
       "         bias=f32[128],\n",
       "         in_features=64,\n",
       "         out_features=128,\n",
       "         use_bias=True\n",
       "       )\n",
       "     ),\n",
       "     activation=<function gelu>,\n",
       "     final_activation=<function <lambda>>,\n",
       "     use_bias=True,\n",
       "     use_final_bias=True,\n",
       "     in_size=64,\n",
       "     out_size=128,\n",
       "     width_size=64,\n",
       "     depth=2\n",
       "   ),\n",
       "   ln1=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   ),\n",
       "   ln2=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   )\n",
       " ),\n",
       " TransformerLayer(\n",
       "   attention=ModulatedAttention(\n",
       "     mha=MultiheadAttention(\n",
       "       query_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       key_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       value_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       output_proj=Linear(\n",
       "         weight=f32[64,60],\n",
       "         bias=None,\n",
       "         in_features=60,\n",
       "         out_features=64,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       dropout=Dropout(p=0.1, inference=False),\n",
       "       num_heads=6,\n",
       "       query_size=64,\n",
       "       key_size=64,\n",
       "       value_size=64,\n",
       "       output_size=64,\n",
       "       qk_size=10,\n",
       "       vo_size=10,\n",
       "       use_query_bias=False,\n",
       "       use_key_bias=False,\n",
       "       use_value_bias=False,\n",
       "       use_output_bias=False\n",
       "     ),\n",
       "     static_proj=Linear(\n",
       "       weight=f32[64,64],\n",
       "       bias=f32[64],\n",
       "       in_features=64,\n",
       "       out_features=64,\n",
       "       use_bias=True\n",
       "     )\n",
       "   ),\n",
       "   ff_block=MLP(\n",
       "     layers=(\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[128,64],\n",
       "         bias=f32[128],\n",
       "         in_features=64,\n",
       "         out_features=128,\n",
       "         use_bias=True\n",
       "       )\n",
       "     ),\n",
       "     activation=<function gelu>,\n",
       "     final_activation=<function <lambda>>,\n",
       "     use_bias=True,\n",
       "     use_final_bias=True,\n",
       "     in_size=64,\n",
       "     out_size=128,\n",
       "     width_size=64,\n",
       "     depth=2\n",
       "   ),\n",
       "   ln1=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   ),\n",
       "   ln2=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   )\n",
       " ),\n",
       " TransformerLayer(\n",
       "   attention=ModulatedAttention(\n",
       "     mha=MultiheadAttention(\n",
       "       query_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       key_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       value_proj=Linear(\n",
       "         weight=f32[60,64],\n",
       "         bias=None,\n",
       "         in_features=64,\n",
       "         out_features=60,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       output_proj=Linear(\n",
       "         weight=f32[64,60],\n",
       "         bias=None,\n",
       "         in_features=60,\n",
       "         out_features=64,\n",
       "         use_bias=False\n",
       "       ),\n",
       "       dropout=Dropout(p=0.1, inference=False),\n",
       "       num_heads=6,\n",
       "       query_size=64,\n",
       "       key_size=64,\n",
       "       value_size=64,\n",
       "       output_size=64,\n",
       "       qk_size=10,\n",
       "       vo_size=10,\n",
       "       use_query_bias=False,\n",
       "       use_key_bias=False,\n",
       "       use_value_bias=False,\n",
       "       use_output_bias=False\n",
       "     ),\n",
       "     static_proj=Linear(\n",
       "       weight=f32[64,64],\n",
       "       bias=f32[64],\n",
       "       in_features=64,\n",
       "       out_features=64,\n",
       "       use_bias=True\n",
       "     )\n",
       "   ),\n",
       "   ff_block=MLP(\n",
       "     layers=(\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[64,64],\n",
       "         bias=f32[64],\n",
       "         in_features=64,\n",
       "         out_features=64,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[128,64],\n",
       "         bias=f32[128],\n",
       "         in_features=64,\n",
       "         out_features=128,\n",
       "         use_bias=True\n",
       "       )\n",
       "     ),\n",
       "     activation=<function gelu>,\n",
       "     final_activation=<function <lambda>>,\n",
       "     use_bias=True,\n",
       "     use_final_bias=True,\n",
       "     in_size=64,\n",
       "     out_size=128,\n",
       "     width_size=64,\n",
       "     depth=2\n",
       "   ),\n",
       "   ln1=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   ),\n",
       "   ln2=LayerNorm(\n",
       "     shape=(64,),\n",
       "     eps=1e-05,\n",
       "     use_weight=True,\n",
       "     use_bias=True,\n",
       "     weight=f32[64],\n",
       "     bias=f32[64]\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.cross_stream_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tss-ml]",
   "language": "python",
   "name": "conda-env-.conda-tss-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
