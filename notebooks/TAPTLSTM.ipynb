{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2f660-7607-4db7-975e-791971031c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is required to run multiple processes on Unity for some reason.\n",
    "from multiprocessing import set_start_method\n",
    "set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f020388-58f0-4da9-933a-6a02129cd624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584f053ea9844e6e968ef82121d30fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Basins:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import optax\n",
    "from pathlib import Path\n",
    "\n",
    "src = Path('../src').resolve()\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)\n",
    "    \n",
    "# Force reload of project files\n",
    "import importlib\n",
    "import data, train, models, evaluate\n",
    "importlib.reload(data)\n",
    "from data import TAPDataset, TAPDataLoader\n",
    "\n",
    "data_dir = Path(\"../data/wqp\")\n",
    "basin_file = data_dir / \"metadata\" / \"site_lists\" / \"sites_test.txt\"\n",
    "# basin_list_file = data_dir / \"metadata\" / \"site_lists\" / \"sites_all.txt\"\n",
    "# basin_file = data_dir / \"metadata\" / \"site_lists\" / \"sites_turb_area1000_n10.txt\"\n",
    "\n",
    "data_args = {'data_dir': data_dir,\n",
    "             'basin_file': basin_file,\n",
    "             'features':{\n",
    "                 'daily':['grfr_q'],\n",
    "                 'irregular':['Blue','Green','Red','Nir','Swir1','Swir2'],\n",
    "                 'static':['wet_pc_s02', 'glc_pc_s11', 'dor_pc_pva', 'soc_th_sav', 'snw_pc_s08',\n",
    "                           'wet_pc_s09', 'fmh_cl_smj', 'glc_pc_s14', 'pnv_pc_s11', 'pac_pc_sse'],\n",
    "                 'target': 'turbidity'},\n",
    "             'time_slice': slice('1979-01-01', '2018-12-31'),\n",
    "             'split_time': np.datetime64('2010-01-01'),\n",
    "             'sequence_length': 30,\n",
    "             'log_norm_cols': ['turbidity', 'grfr_q'],\n",
    "             'clip_target_to_zero': True}\n",
    "\n",
    "dataset = TAPDataset(**data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd9e66e-0054-4dab-8dc5-0c6434c17fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader using 1 parallel CPU worker(s).\n",
      "Batch sharding set to 1 cpu(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlanghorst_umass_edu/.conda/envs/tss-ml/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc1ff623aa744c7be8349fc05795d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:001:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected segmentation fault encountered in worker.\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1928876) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1928876) is killed by signal: Segmentation fault. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m trainer_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_func\u001b[39m\u001b[38;5;124m'\u001b[39m: TAPLSTM,\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m'\u001b[39m: model_args,\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataloader\u001b[39m\u001b[38;5;124m'\u001b[39m: dataloader,\n\u001b[1;32m     32\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_schedule\u001b[39m\u001b[38;5;124m'\u001b[39m: lr_schedule,\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: num_epochs,\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_args)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pi_kandread_umass_edu/tss-ml/src/train.py:95\u001b[0m, in \u001b[0;36mTrainer.start_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 95\u001b[0m     loss, bad_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     info_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(info_str)\n",
      "File \u001b[0;32m/work/pi_kandread_umass_edu/tss-ml/src/train.py:140\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m bad_grads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvanishing\u001b[39m\u001b[38;5;124m'\u001b[39m: {}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexploding\u001b[39m\u001b[38;5;124m'\u001b[39m:{}}\n\u001b[1;32m    139\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquiet, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m basins, dates, batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader\u001b[38;5;241m.\u001b[39mshard_batch(batch)\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1111\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1109\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1111\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/tss-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1928876) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "importlib.reload(train)\n",
    "from models import TAPLSTM, EALSTM, LSTM\n",
    "from train import Trainer\n",
    "\n",
    "loader_args = {'shuffle': True,\n",
    "               'batch_size': 16,\n",
    "               'data_subset': 'pre-train',\n",
    "               'num_workers': 1,\n",
    "               'pin_memory': True} \n",
    "dataloader = TAPDataLoader(dataset, **loader_args)\n",
    "\n",
    "model_args = {'daily_in_size': len(dataset.daily_features),\n",
    "              'irregular_in_size': len(dataset.irregular_features),\n",
    "              'static_in_size': len(dataset.static_features),\n",
    "              'out_size': 1,\n",
    "              'hidden_size': 16,\n",
    "              'dropout': 0.4,\n",
    "              'seed': 0}\n",
    "# model_args = {'dynamic_in_size': len(data_args['features']['daily']),\n",
    "#               'static_in_size': len(dataset.static_features),\n",
    "#               'out_size': 1,\n",
    "#               'hidden_size': 16,\n",
    "#               'dropout': 0.4,\n",
    "#               'seed': 0}\n",
    "\n",
    "num_epochs = 5\n",
    "lr_schedule = optax.exponential_decay(0.01, num_epochs, 0.01)\n",
    "trainer_args = {'model_func': TAPLSTM,\n",
    "                'model_args': model_args,\n",
    "                'dataloader': dataloader,\n",
    "                'lr_schedule': lr_schedule,\n",
    "                'num_epochs': num_epochs,\n",
    "                'max_grad_norm': 2}\n",
    "\n",
    "trainer = Trainer(**trainer_args)\n",
    "trainer.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe339df-1e23-4067-9107-a774c6884a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_epochs = 150\n",
    "\n",
    "trainer.load_state('epoch100')\n",
    "loader_args['data_subset'] = 'train'\n",
    "trainer.dataloader = TAPDataLoader(dataset, **loader_args)\n",
    "trainer.lr_schedule = optax.exponential_decay(0.01, trainer.epoch+more_epochs, 0.001, transition_begin=trainer.epoch)\n",
    "trainer.num_epochs += more_epochs\n",
    "trainer.freeze_components('tealstm_i',True)\n",
    "trainer.start_training() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0777957-11f3-49db-b4a4-67d729527ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = optax.exponential_decay(0.01, trainer.epoch+num_epochs, 0.001, transition_begin=trainer.epoch)\n",
    "x = np.linspace(0,num_epochs*2)\n",
    "y = lr_schedule(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ca8c8-9369-4020-8b90-a935d979b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(evaluate)\n",
    "from evaluate import predict, get_all_metrics\n",
    "\n",
    "basin = np.random.choice(basin_list).tolist()\n",
    "\n",
    "loader_args['data_subset'] = 'test'\n",
    "loader_args['basin_subset'] =  basin\n",
    "loader_args['num_workers'] = 0 # Faster for small runs\n",
    "dataloader = TAPDataLoader(dataset, **loader_args)\n",
    "\n",
    "results = predict(trainer.model, dataloader, denormalize=True)\n",
    "results['pred'] = results['pred'] * (results['pred']>0) #Clip predictions to 0\n",
    "metrics = get_all_metrics(results['obs'],results['pred'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35e0e8-6279-4f8b-9559-24f543f7bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e119cc9-dc63-411f-a3a8-0a17c5865b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values and predictions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "results['pred'].plot(ax=ax)\n",
    "results['obs'].plot(ax=ax,linestyle='None',marker='.')\n",
    "\n",
    "metrics = get_all_metrics(results['obs'],results['pred'])\n",
    "\n",
    "plt.title(f\"Basin: {basin}, KGE: {metrics['kge']:0.4f}\")\n",
    "plt.legend()\n",
    "plt.ylim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fec15c-904c-4b45-b531-f36c771772c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot.scatter('obs','pred')\n",
    "plt.gca().axis('square')\n",
    "plt.xlim([0,20])\n",
    "plt.ylim([0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcf4b4-1d1b-49e1-af29-91faeb56590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d973e14-3b94-4f55-b971-e7aebf7887e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe46d7-39ac-421b-94bd-cf015c31e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import make_step\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# See if we can recreate the error... \n",
    "data = trainer.load_state(\"exceptions/epoch130_exception0\")\n",
    "for i in trange(1000):\n",
    "    make_step(trainer.model, data['batch'], trainer.opt_state, trainer.optim,\n",
    "              trainer.filter_spec, loss_name=\"mse\", max_grad_norm=None, l2_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4dcd7-9d86-4b5b-9334-b3bd2ff06494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tss-ml]",
   "language": "python",
   "name": "conda-env-.conda-tss-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
