# ┌────────────────────────────────┐ #
# │             DATA               │ #
# └────────────────────────────────┘ #
# Needs to at least have the following structure:
# data_dir/
# ├── attributes/
# │   └── attributes.csv
# └── time_series/
#     ├── xxxx.nc
#     └── ... (one .nc file for each site)
data_dir: "/work/pi_kandread_umass_edu/tss-ml/data/wqp"

# Relative to data_dir. A plain text list of sites to use. One site per line.
basin_file: "metadata/site_lists/sites_test.txt" 

# Dictionary of exact column names in the netcdf files
features:
    # Daily (or consistent timescale) features in the netcdf files. Can not contain NaN data.
    daily:
      - "grfr_q"
    # Irregularly-timed features in the netcdf files. NaN values are replaced and/or masked.
    irregular: 
      - "Blue"
      - "Green"
      - "Red"
      - "Nir"
      - "Swir1"
      - "Swir2"
    #Static basin features from the csv file. If none, all are used.
    static: 
    # Target feature
    target: "turbidity"
    
# Start and end of join train and test periods
time_slice: ["1979-01-01", "2018-12-31"]

# When to split train / test. Train is always the first period.
split_time: 2010-01-01

log_norm_cols:
    - "turbidity"
    - "grfr_q"
    
# range_norm_cols:

clip_target_to_zero: true

# ┌────────────────────────────────┐ #
# │           DataLoader           │ #
# └────────────────────────────────┘ #
shuffle: true
batch_size: 32

# pre-train, train, or test
data_subset: "train"

# number of parallel cpus used for data loading
# 0 is faster than 1 when there are a small number of batches per epoch
num_workers: 1

# XLA backend: cpu, gpu, or tpu. defaults to gpu if available, then cpu.
backend:

# Number of backed devices to use. defaults to use all available.
num_devices: 

# Static memory allocation for batches on device
pin_memory: true

# ┌────────────────────────────────┐ #
# │             Model              │ #
# └────────────────────────────────┘ #
# Model name. taplstm, ealstm, tealstm, eatransformer
model: "taplstm"

sequence_length: 30

# Different for eatransformer model
model_args:
    hidden_size: 64
    dropout: 0.4
    seed: 0
    
# ┌────────────────────────────────┐ #
# │            Trainer             │ #
# └────────────────────────────────┘ #
num_epochs: 10

# Parameters for lr scheduler (https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html#optax.exponential_decay)
# learning_rate(epoch) = initial_lr * (decay_rate ** (epoch / num_epochs))
initial_lr: 0.01
decay_rate: 0.01

# Params passed to model steps during training.
step_kwargs:
    # mse and mae only right now
    loss: "mse" 
    # Gradient clipping to this norm
    max_grad_norm: 2

# ┌────────────────────────────────┐ #
# │            Outputs             │ #
# └────────────────────────────────┘ #
# Use to declutter the slurm output (removes tqdm)
quiet: true

log: true

# How frequently (in epochs) to save training progress
log_interval: 5
